@article{wang_deepvo_nodate,
	title = {{DeepVO}: {Towards} end-to-end visual odometry with deep {Recurrent} {Convolutional} {Neural} {Networks}},
	author = {Wang, Sen and Clark, Ronald and Wen, Hongkai and Trigoni, Niki},
}

@ARTICLE{swformer,
	author={Wu, Zhigang and Zhu, Yaohui},
	journal={IEEE Robotics and Automation Letters}, 
	title={SWformer-VO: A Monocular Visual Odometry Model Based on Swin Transformer}, 
	year={2024},
	volume={9},
	number={5},
	pages={4766-4773},
	keywords={Transformers;Visual odometry;Cameras;Training;Odometry;Image segmentation;Deep learning;Deep learning;monocular visual odometry;transformer},
	doi={10.1109/LRA.2024.3384911}}


@INPROCEEDINGS{10610979,
	author={Xin, Shuo and Zhang, Zhen and Wang, Mengmeng and Hou, Xiaojun and Guo, Yaowei and Kang, Xiao and Liu, Liang and Liu, Yong},
	booktitle={2024 IEEE International Conference on Robotics and Automation (ICRA)}, 
	title={Multi-modal 3D Human Tracking for Robots in Complex Environment with Siamese Point-Video Transformer}, 
	year={2024},
	volume={},
	number={},
	pages={337-344},
	keywords={Point cloud compression;Three-dimensional displays;Target tracking;Laser radar;Jitter;Transformers;Proposals},
	doi={10.1109/ICRA57147.2024.10610979}}

@article{wu2024moving,
	title={Moving event detection from LiDAR point streams},
	author={Wu, Huajie and Li, Yihang and Xu, Wei and Kong, Fanze and Zhang, Fu},
	journal={nature communications},
	volume={15},
	number={1},
	pages={345},
	year={2024},
	publisher={Nature Publishing Group UK London}
}